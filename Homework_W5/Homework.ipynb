{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 16:05:41 WARN Utils: Your hostname, DESKTOP-5VNGFU5 resolves to a loopback address: 127.0.1.1; using 172.26.218.188 instead (on interface eth0)\n",
      "23/02/28 16:05:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 16:05:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName('Spark_Homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    ".option('inferSchema', 'true')\\\n",
    ".option('header', 'true')\\\n",
    ".csv('fhvhv_tripdata_2021-06.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".schema(schema)\\\n",
    ".option('header', 'true')\\\n",
    ".csv('fhvhv_tripdata_2021-06.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(12).write.parquet('data/fhv_tripdata/2021/06/', mode='overwrite')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2021-06-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('trips_fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT count(*) from trips_fhv\n",
    "where date_trunc(\"Day\", pickup_datetime) = '2021-06-15'\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|  max(trip_time)|\n",
      "+----------------+\n",
      "|66.8788888888889|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "with trip_time_table as (\n",
    "    SELECT (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime))/3600 AS trip_time\n",
    "    from trips_fhv)\n",
    "\n",
    "SELECT MAX(trip_time) from trip_time_table\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------+------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|DiffInSeconds|       DiffInHours|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------+------------------+\n",
      "|              B02872|2021-06-25 13:55:41|2021-06-28 08:48:25|          98|         265|      N|                  null|       240764|  66.8788888888889|\n",
      "|              B02765|2021-06-22 12:09:45|2021-06-23 13:42:44|         188|         198|      N|                  null|        91979|25.549722222222222|\n",
      "|              B02879|2021-06-27 10:32:29|2021-06-28 06:31:20|          78|         169|      N|                  null|        71931|19.980833333333333|\n",
      "|              B02800|2021-06-26 22:37:11|2021-06-27 16:49:01|         263|          36|      N|                  null|        65510|18.197222222222223|\n",
      "|              B02682|2021-06-23 20:40:43|2021-06-24 13:08:44|           3|         247|      N|                  null|        59281|16.466944444444444|\n",
      "|              B02869|2021-06-23 22:03:31|2021-06-24 12:19:39|         186|         216|      N|                  null|        51368|14.268888888888888|\n",
      "|              B02877|2021-06-24 23:11:00|2021-06-25 13:05:35|         181|          61|      N|                  null|        50075|13.909722222222221|\n",
      "|              B02765|2021-06-04 20:56:02|2021-06-05 08:36:14|          53|         252|      N|                  null|        42012|             11.67|\n",
      "|              B02617|2021-06-27 07:45:19|2021-06-27 19:07:16|         187|         245|      N|                  null|        40917|11.365833333333333|\n",
      "|              B02880|2021-06-20 17:05:12|2021-06-21 04:04:16|         144|         231|      N|                  null|        39544|10.984444444444444|\n",
      "|              B02866|2021-06-01 12:25:29|2021-06-01 22:41:32|          87|         265|      N|                  null|        36963|           10.2675|\n",
      "|              B02510|2021-06-01 12:01:46|2021-06-01 21:59:45|          17|          37|      N|                  null|        35879| 9.966388888888888|\n",
      "|              B02882|2021-06-28 13:13:59|2021-06-28 23:11:58|          39|         131|      N|                  null|        35879| 9.966388888888888|\n",
      "|              B02510|2021-06-27 03:52:14|2021-06-27 13:30:30|          42|         242|      N|                  null|        34696| 9.637777777777778|\n",
      "|              B02510|2021-06-18 08:50:29|2021-06-18 18:27:57|          39|         216|      N|                  null|        34648| 9.624444444444444|\n",
      "|              B02510|2021-06-08 16:38:14|2021-06-09 02:07:03|         106|         102|      N|                  null|        34129| 9.480277777777777|\n",
      "|              B02800|2021-06-11 23:26:20|2021-06-12 08:54:38|         132|         140|      N|                  null|        34098| 9.471666666666666|\n",
      "|              B02510|2021-06-15 06:47:22|2021-06-15 16:11:30|         141|         232|      N|                  null|        33848| 9.402222222222223|\n",
      "|              B02510|2021-06-25 02:32:24|2021-06-25 11:56:01|          87|         145|      N|                  null|        33817| 9.393611111111111|\n",
      "|              B02764|2021-06-04 17:41:23|2021-06-05 03:04:00|          26|         238|      N|                  null|        33757| 9.376944444444444|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "df \\\n",
    "    .withColumn(\"DiffInSeconds\", F.col(\"dropoff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\"))\\\n",
    "    .withColumn(\"DiffInHours\",F.col(\"DiffInSeconds\")/3600)\\\n",
    "    .sort(F.col(\"DiffInHours\").desc())\\\n",
    "    .show()   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Prefect-sHsPF6vH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0e39738d477f7f731b587c7cd0210656fa5aa92f25f55fead05025c28c9ac0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
